{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z_wN2v1RT1F"
   },
   "source": [
    "# **Assignment-1 for CS60075: Natural Language Processing**\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
    "\n",
    "#### Date of Announcement: 4th Sept, 2021\n",
    "#### Deadline for Submission: 11.59pm on Sunday, 12th Sept, 2021 \n",
    "\n",
    "#### (**NOTE**: Submit a .zip file, containing this .ipynb file, named as `<Your_Roll_Number>_Assn1_NLP_A21.ipynb` and the raw text corpus named `<Your_Roll_Number>_Assn1_rawCorpus.txt`. For example, if your roll number is 20XX12Y45, name the .ipynb file as `20XX12Y45_Assn1_NLP_A21.ipynb`. Name the .zip as `<Your_Roll_Number>_Assn1_NLP_A21.zip`. Write your code in the respective designated portion of the .ipynb. Also before submitting, make sure that all the outputs of your code are present in the .ipynb file itself.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a35tmEySCx7"
   },
   "source": [
    "### **Submission Details:**\n",
    "Name: Vibhanshu Ranjan\n",
    "\n",
    "Roll No.: 18EE35032\n",
    "\n",
    "Department: Electrical Engineering\n",
    "\n",
    "Email-ID: vranjan198351@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9weHMmyd8fnq"
   },
   "source": [
    "## **Reading a Raw Text Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmSy_LOK2aGQ"
   },
   "source": [
    "Retrieve & save raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "rku6rV2ORpZA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151735"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To construct your corpus, retrieve (through Python code) Chapter I to Chapter X,\n",
    "# both inclusive, from the link below:\n",
    "# \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
    "# Save this corpus in a text file, named as 'rawCorpus.txt'\n",
    "# Print the total number of characters in the text file \n",
    "\n",
    "# *** Write code ***\n",
    "\n",
    "from urllib.request import urlopen\n",
    "url = \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
    "page = urlopen(url)\n",
    "html_bytes = page.read()\n",
    "html = html_bytes.decode(\"utf-8\")\n",
    "start_index = html.find(\"CHAPTER I.\") #finding index of chapter I.\n",
    "end_index = html.find(\"CHAPTER XI.\")  #finding index of chapter XI.\n",
    "title = html[start_index:end_index]   #including contents between chapter 1 and chapter 11 i.e. from chapter 11 text will be ignored\n",
    "file = open('18EE35032_Assn1_rawCorpus.txt','w+')\n",
    "file.write(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "151735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KZIOy0Y2hzQ"
   },
   "source": [
    "Read the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "DsdBJa_l2l7g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of characters in read dataset: 151735\n"
     ]
    }
   ],
   "source": [
    "# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\n",
    "# *** Write code ***\n",
    "file = open(\"18EE35032_Assn1_rawCorpus.txt\", \"r\")\n",
    "rawReadCorpus = file.read()\n",
    "print (\"Total # of characters in read dataset: {}\".format(len(rawReadCorpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhkmGsSoV0zG"
   },
   "source": [
    "## **Installing NLTK**\n",
    "\n",
    "The Natural Language Toolkit ([NLTK](https://www.nltk.org/)) is a Python module that is intended to support research and teaching in NLP or closely related areas. \n",
    "\n",
    "Detailed installation instructions to install NLTK can be found at this [link](https://www.nltk.org/install.html).\n",
    "\n",
    "To ensure uniformity, we suggest to use **python3**. You can download Anaconda3 and create a separate environment to do this assignment, eg.\n",
    "```bash\n",
    "conda create -n myenv python=3.6\n",
    "conda activate myenv\n",
    "```\n",
    "\n",
    "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. Subsequently, you can install NLTK through the following commands:\n",
    "```bash\n",
    "sudo pip3 install nltk \n",
    "python3 \n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utKtZeHq4N98"
   },
   "source": [
    "## **Preprocessing the corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-LSUX__82Ff"
   },
   "source": [
    "**Tokenize into words and sentences, using NLTK library:** Using the NLTK modules imported above, retrieve a case-insensitive preprocessed model. Make sure to take care of words like \"\\_will\\_\" (that should ideally appear as \"will\"), \"wouldn't\" (that should ideally appear as a single word, and not multiple tokens) and other occurences of special cases that you find in the raw corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2g7eO4Dm4jIn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bolkbam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "cWIzYXyz9Zt_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing first 5 sentences of preprocessed corpus-----------------\n",
      "\n",
      "chapter i \n",
      "\n",
      "treats of the place where oliver twist was born and of the circumstances attending his birth among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter \n",
      "\n",
      "for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country \n",
      "\n",
      "although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred \n",
      "\n",
      "the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respirationa troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter \n",
      "\n",
      "Printing first 5 words of preprocessed corpus-------------------\n",
      "\n",
      "chapter \n",
      "\n",
      "i \n",
      "\n",
      "treats \n",
      "\n",
      "of \n",
      "\n",
      "the \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *** Write code for preprocessing the corpus ***\n",
    "\n",
    "sents = nltk.sent_tokenize(rawReadCorpus)\n",
    "for i in range(len(sents)):\n",
    "    sents[i] = sents[i].lower()  #lowercasing sentences\n",
    "    sents[i] = re.sub(r\"[^a-zA-Z0-9 '\\n' ]+\", \"\", sents[i])  #Removing punctuations\n",
    "    sents[i] = re.sub(r\"[^a-zA-Z0-9]+\", \" \", sents[i]) \n",
    "    \n",
    "\"\"\"\"Note:------------- Removing punctuations because in the predictions mostly , and . are coming so getting the idea\n",
    "of prediction of words i removed it\"\"\"\n",
    "\n",
    "words = [] # contain tokens/words\n",
    "for s in sents:\n",
    "    s = word_tokenize(s) #tokenizing sentences into words\n",
    "    for i in s:\n",
    "        words.append(i)\n",
    "        \n",
    "        \n",
    "\n",
    "# Print first 5 sentences of your preprocessed corpus *** Write code ***\n",
    "print(\"Printing first 5 sentences of preprocessed corpus-----------------\\n\")\n",
    "for i in range(5):\n",
    "    print(sents[i],'\\n')\n",
    "    \n",
    "    \n",
    "# Print first 5 words/tokens of your preprocessed corpus *** Write code ***\n",
    "print(\"Printing first 5 words of preprocessed corpus-------------------\\n\")\n",
    "for i in range(5):\n",
    "    print(words[i],'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ75_a1QL70J"
   },
   "source": [
    "**Perform the following tasks for the given corpus:**\n",
    "1. Print the average number of tokens per sentence.\n",
    "2. Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
    "3. Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "pyG0g3oSADmV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bolkbam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ydHIxC7lG7Py"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. no of tokens per sentence: 23.797260273972604 \n",
      "\n",
      "Lenght of the Longest and the Shortest sentence respectively , that contains the word Oliver:  114 , 2 \n",
      "\n",
      "Number of unique tokens in the corpus, after stopword removal: 4210\n"
     ]
    }
   ],
   "source": [
    "# *** Write code for the 2 tasks above ***\n",
    "\n",
    "\n",
    "\n",
    "# Printing the average number of tokens per sentence\n",
    "len_w = len(words) #total no of words/tokens\n",
    "len_s = len(sents) #total no of sentences\n",
    "avg = len_w/len_s\n",
    "print(\"Avg. no of tokens per sentence:\",avg,'\\n')\n",
    "\n",
    "\n",
    "# Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
    "longest = 0\n",
    "shortest = 10000000\n",
    "\n",
    "for sent in sents:\n",
    "    lis = sent.split()\n",
    "    if(lis.count('oliver')>0):  #Oliver is case insensitive\n",
    "        if(longest < len(lis)):\n",
    "            longest = len(lis)    \n",
    "        if(shortest > len(lis)):\n",
    "            shortest = len(lis)\n",
    "            \n",
    "print('Lenght of the Longest and the Shortest sentence respectively , that contains the word Oliver: ',longest,',',shortest,'\\n')\n",
    "\n",
    "\n",
    "# Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive).\n",
    "stop_words = set(stopwords.words('english'))\n",
    "unique_words = []\n",
    "\n",
    "for r in words:\n",
    "    if ((not r in stop_words) and (not r in unique_words)):\n",
    "        unique_words.append(r)\n",
    "        \n",
    "print('Number of unique tokens in the corpus, after stopword removal:',len(unique_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5RiDR7TJjKX"
   },
   "source": [
    "## **Language Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJeTSt8HM95L"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the given corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "\n",
    "2. **List the top 10 bigrams, trigrams**\n",
    "(Additionally remove those items which contain only articles, prepositions, determiners eg. \"of the\", \"in a\", etc. List top-10 bigrams/trigrams in both the original and processed models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "DlPXGvVaR-ka"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of n-grams:\n",
      "-------------------------\n",
      "--> UNIGRAMS: \n",
      "['chapter', 'i', 'treats', 'of', 'the'] ...\n",
      "\n",
      "--> BIGRAMS: \n",
      "[('chapter', 'i'), ('treats', 'of'), ('of', 'the'), ('the', 'place'), ('place', 'where')] ...\n",
      "\n",
      "--> TRIGRAMS: \n",
      "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
      "\n",
      "Sample of n-grams after processing:\n",
      "-------------------------\n",
      "--> UNIGRAMS: \n",
      "['chapter', 'treats', 'place', 'oliver', 'twist'] ...\n",
      "\n",
      "--> BIGRAMS: \n",
      "[('chapter', 'i'), ('treats', 'of'), ('the', 'place'), ('place', 'where'), ('where', 'oliver')] ...\n",
      "\n",
      "--> TRIGRAMS: \n",
      "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
      "\n",
      "Top 10 unigrams_freqDist\n",
      "\n",
      "the   1701  \n",
      "and   856  \n",
      "a   713  \n",
      "of   673  \n",
      "to   617  \n",
      "his   455  \n",
      "he   449  \n",
      "in   441  \n",
      "was   368  \n",
      "oliver   277  \n",
      "\n",
      "\n",
      "Top 10 unigrams_Processed_freqDist\n",
      "\n",
      "oliver   277  \n",
      "said   212  \n",
      "mr   191  \n",
      "bumble   123  \n",
      "gentleman   102  \n",
      "old   89  \n",
      "would   77  \n",
      "sowerberry   77  \n",
      "replied   74  \n",
      "boy   74  \n",
      "\n",
      "\n",
      "Top 10 bigrams_freqDist\n",
      "\n",
      "('of', 'the')   162  \n",
      "('in', 'the')   127  \n",
      "('mr', 'bumble')   107  \n",
      "('to', 'the')   91  \n",
      "('said', 'the')   90  \n",
      "('he', 'had')   67  \n",
      "('he', 'was')   62  \n",
      "('on', 'the')   60  \n",
      "('in', 'a')   55  \n",
      "('with', 'a')   54  \n",
      "\n",
      "\n",
      "Top 10 bigrams_Processed_freqDist\n",
      "\n",
      "('mr', 'bumble')   107  \n",
      "('said', 'the')   90  \n",
      "('the', 'old')   53  \n",
      "('old', 'gentleman')   39  \n",
      "('the', 'undertaker')   36  \n",
      "('said', 'mr')   35  \n",
      "('the', 'boy')   35  \n",
      "('the', 'gentleman')   33  \n",
      "('the', 'jew')   33  \n",
      "('mr', 'sowerberry')   32  \n",
      "\n",
      "\n",
      "Top 10 trigrams_freqDist\n",
      "\n",
      "('the', 'old', 'gentleman')   29  \n",
      "('gentleman', 'in', 'the')   22  \n",
      "('the', 'gentleman', 'in')   20  \n",
      "('the', 'white', 'waistcoat')   20  \n",
      "('said', 'mr', 'bumble')   19  \n",
      "('in', 'the', 'white')   18  \n",
      "('said', 'the', 'gentleman')   14  \n",
      "('said', 'the', 'undertaker')   14  \n",
      "('said', 'the', 'jew')   14  \n",
      "('sir', 'replied', 'oliver')   12  \n",
      "\n",
      "\n",
      "Top 10 trigrams_Processed_freqDist\n",
      "\n",
      "('the', 'old', 'gentleman')   29  \n",
      "('gentleman', 'in', 'the')   22  \n",
      "('the', 'gentleman', 'in')   20  \n",
      "('the', 'white', 'waistcoat')   20  \n",
      "('said', 'mr', 'bumble')   19  \n",
      "('in', 'the', 'white')   18  \n",
      "('said', 'the', 'gentleman')   14  \n",
      "('said', 'the', 'undertaker')   14  \n",
      "('said', 'the', 'jew')   14  \n",
      "('sir', 'replied', 'oliver')   12  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "\n",
    "for content in (sents): # *** Write code ***\n",
    "    contents = word_tokenize(content) #tokenizing sentence in words\n",
    "    unigrams.extend(contents)  \n",
    "    bigrams.extend(ngrams(contents,2)) \n",
    "    ##similar for trigrams \n",
    "    # *** Write code ***\n",
    "    trigrams.extend(ngrams(contents,3))\n",
    "\n",
    "print (\"Sample of n-grams:\\n\" + \"-------------------------\")\n",
    "print (\"--> UNIGRAMS: \\n\" + str(unigrams[:5]) + \" ...\\n\")\n",
    "print (\"--> BIGRAMS: \\n\" + str(bigrams[:5]) + \" ...\\n\")\n",
    "print (\"--> TRIGRAMS: \\n\" + str(trigrams[:5]) + \" ...\\n\")\n",
    "\n",
    "\n",
    "# list of unigram, bigram & trigram after removing those that \n",
    "# totally contain only articles, prepositions, determiners\n",
    "# Eg. For bigrams, don't remove items like (\"a\", \"boy\") --> where not all are \n",
    "#     articles, prepositions, determiners\n",
    "#     But remove items like (\"in\", \"the\") --> where all are articles, prepositions, determiners\n",
    "# Similarly, for unigrams and trigrams\n",
    "unigrams_Processed = [p for p in unigrams if p not in stop_words] # *** Write code ***\n",
    "\n",
    "bigrams_Processed = [] # *** Write code ***\n",
    "for p in bigrams:\n",
    "    for t in p:\n",
    "        if t not in stop_words:    # if any of the word/token is not in stop_words then include it in bigrams_Processed\n",
    "            bigrams_Processed.append(p)\n",
    "            break\n",
    "    \n",
    "trigrams_Processed = [] # *** Write code ***\n",
    "for p in trigrams:\n",
    "    for t in p:\n",
    "        if t not in stop_words:    # if any of the word/token is not in stop_words then include it \n",
    "            trigrams_Processed.append(p)\n",
    "            break\n",
    "    \n",
    "print (\"Sample of n-grams after processing:\\n\" + \"-------------------------\")\n",
    "print (\"--> UNIGRAMS: \\n\" + str(unigrams_Processed[:5]) + \" ...\\n\")\n",
    "print (\"--> BIGRAMS: \\n\" + str(bigrams_Processed[:5]) + \" ...\\n\")\n",
    "print (\"--> TRIGRAMS: \\n\" + str(trigrams_Processed[:5]) + \" ...\\n\")\n",
    "\n",
    "def get_ngrams_freqDist(n, ngramList):\n",
    "    #This function computes the frequency corresponding to each ngram in ngramList \n",
    "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
    "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
    "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
    "    \n",
    "    # *** Write code ***\n",
    "    ngram_freq_dict = nltk.FreqDist(ngramList)\n",
    "    return ngram_freq_dict\n",
    "\n",
    "unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\n",
    "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
    "bigrams_freqDist = get_ngrams_freqDist(2, bigrams)\n",
    "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
    "trigrams_freqDist = get_ngrams_freqDist(3, trigrams)\n",
    "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)  \n",
    "\n",
    "#Printing top 10 ngrams having highest frequency\n",
    "def print_fun(lis):\n",
    "    k = Counter(lis)\n",
    "    ten = k.most_common(10)\n",
    "    for i in ten:\n",
    "      print(i[0],\" \",i[1],\" \")\n",
    "    print('\\n')\n",
    "\n",
    "# Print top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
    "# *** Write code ***\n",
    "print('Top 10 unigrams_freqDist\\n')\n",
    "print_fun(unigrams_freqDist)\n",
    "\n",
    "# Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
    "# *** Write code ***\n",
    "print('Top 10 unigrams_Processed_freqDist\\n')\n",
    "print_fun(unigrams_Processed_freqDist)\n",
    "\n",
    "# Print top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
    "# *** Write code ***\n",
    "print('Top 10 bigrams_freqDist\\n')\n",
    "print_fun(bigrams_freqDist)\n",
    "\n",
    "\n",
    "# Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
    "# *** Write code ***\n",
    "print('Top 10 bigrams_Processed_freqDist\\n')\n",
    "print_fun(bigrams_Processed_freqDist)\n",
    "\n",
    "        \n",
    "# Print top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
    "# *** Write code ***\n",
    "print('Top 10 trigrams_freqDist\\n')\n",
    "print_fun(trigrams_freqDist)\n",
    "\n",
    "\n",
    "# Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
    "# *** Write code ***\n",
    "print('Top 10 trigrams_Processed_freqDist\\n')\n",
    "print_fun(trigrams_Processed_freqDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqu8nVV7NREo"
   },
   "source": [
    "## **Next three words' Prediction using Smoothed Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2vnIM26b2WA"
   },
   "source": [
    "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
    "That is, pretend we saw each word one more time than we did.\n",
    "\n",
    "You have two tasks here.\n",
    "\n",
    "First, compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist you calculated above (use the unprocessed models). Second, using these smoothed models, predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\n",
    "\n",
    "As for example, for the string 'Raj has a' the answers can be as below: \n",
    "\n",
    "(1) Raj has a **beautiful red car**\n",
    "\n",
    "(2) Raj has a **charismatic magnetic personality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "qAGB1_S8NThy"
   },
   "outputs": [],
   "source": [
    "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
    "testSent2 = \"They made room for the stranger, but he sat down\"\n",
    "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "yLY1ymH-ZuJu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of', 'the')   0.009575280502849087  \n",
      "('in', 'the')   0.007623131439461616  \n",
      "('mr', 'bumble')   0.0065292303971948495  \n",
      "('said', 'the')   0.005494505494505495  \n",
      "('to', 'the')   0.005422290328284317  \n",
      "('he', 'had')   0.00404785999166617  \n",
      "('he', 'was')   0.0037502232275730698  \n",
      "('on', 'the')   0.0036971937693193527  \n",
      "('in', 'a')   0.003335120004764457  \n",
      "('with', 'a')   0.0033132530120481927  \n",
      "\n",
      "\n",
      "('the', 'old', 'gentleman')   0.0013531799729364006  \n",
      "('gentleman', 'in', 'the')   0.0010387498870924035  \n",
      "('the', 'white', 'waistcoat')   0.0009485951757159635  \n",
      "('the', 'gentleman', 'in')   0.0009480812641083522  \n",
      "('said', 'mr', 'bumble')   0.0009028530155290719  \n",
      "('in', 'the', 'white')   0.0008541629203380687  \n",
      "('said', 'the', 'gentleman')   0.0006754626919439817  \n",
      "('said', 'the', 'undertaker')   0.0006754626919439817  \n",
      "('said', 'the', 'jew')   0.0006754626919439817  \n",
      "('sir', 'replied', 'oliver')   0.0005873317068762989  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *** Write code ***\n",
    "\n",
    "# Computing the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist (using unprocessed models).\n",
    "V1 = len(bigrams_freqDist)  #Number of unique 2-grams\n",
    "V2 = len(trigrams_freqDist) #Number of unique 3-grams\n",
    "\n",
    "bigram_prob = {}\n",
    "trigram_prob = {}\n",
    "\n",
    "for key in bigrams_freqDist:\n",
    "    bigram_prob[key] = (bigrams_freqDist[key]+1)/(unigrams_freqDist[(key[0])]+V1)\n",
    "    \n",
    "for key in trigrams_freqDist:\n",
    "    trigram_prob[key] = (trigrams_freqDist[key]+1)/(bigrams_freqDist[(key[0],key[1])]+V2)\n",
    "    \n",
    "\n",
    "#Just to check     \n",
    "# printing probability of top ten bigrams post smoothing\n",
    "print_fun(bigram_prob)\n",
    "\n",
    "# printing probablity of top ten trigrams post smoothing\n",
    "print_fun(trigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Predictions:-------------------------------------------------------------\n",
      "\n",
      "\n",
      "Prediction of 10  highest probability for str1\n",
      "\n",
      "('was', 'in', 'the')   1.5390357777246282e-08  \n",
      "('was', 'a', 'very')   1.2423650236880854e-08  \n",
      "('was', 'a', 'great')   1.0583109461046655e-08  \n",
      "('was', 'a', 'good')   9.202703879171003e-09  \n",
      "('was', 'to', 'the')   8.514391311785893e-09  \n",
      "('was', 'the', 'old')   8.052793047960226e-09  \n",
      "('was', 'a', 'little')   7.362163103336803e-09  \n",
      "('was', 'a', 'few')   7.362163103336803e-09  \n",
      "('was', 'in', 'a')   6.733281527545249e-09  \n",
      "('said', 'the', 'old')   5.870686995281663e-09  \n",
      "\n",
      "\n",
      "\n",
      "Prediction of 10  highest  probability for str2\n",
      "\n",
      "('the', 'old', 'gentleman')   7.0976561703264826e-09  \n",
      "('to', 'the', 'old')   5.9312249624976395e-09  \n",
      "('in', 'the', 'old')   4.169318198463357e-09  \n",
      "('to', 'the', 'undertaker')   4.063987474303938e-09  \n",
      "('to', 'the', 'boy')   3.954149974998426e-09  \n",
      "('to', 'the', 'gentleman')   3.734474976387403e-09  \n",
      "('to', 'the', 'jew')   3.734474976387403e-09  \n",
      "('to', 'the', 'beadle')   3.4049624784708674e-09  \n",
      "('to', 'the', 'board')   2.9656124812488197e-09  \n",
      "('the', 'gentleman', 'in')   2.9024862137962903e-09  \n",
      "\n",
      "\n",
      "\n",
      "Prediction  of 10  highest probability for str3\n",
      "\n",
      "('the', 'old', 'gentleman')   2.250036913238397e-08  \n",
      "('the', 'gentleman', 'in')   9.201208067122816e-09  \n",
      "('the', 'old', 'woman')   5.625092283095992e-09  \n",
      "('the', 'white', 'waistcoat')   5.291197526321083e-09  \n",
      "('the', 'old', 'lady')   2.812546141547996e-09  \n",
      "('the', 'old', 'gentlemans')   2.812546141547996e-09  \n",
      "('the', 'gentleman', 'with')   2.4772483257638353e-09  \n",
      "('the', 'boy', 'who')   2.2520918665809793e-09  \n",
      "('the', 'boy', 'said')   2.2520918665809793e-09  \n",
      "('the', 'old', 'womans')   2.2500369132383967e-09  \n",
      "\n",
      "\n",
      "Trigram Predictions:-------------------------------------------------------------\n",
      "\n",
      "\n",
      "Prediction of 10 highest probability for str1\n",
      "\n",
      "('sat', 'down', 'to')   1.6628697721508572e-12  \n",
      "('sat', 'down', 'and')   1.1085798481005716e-12  \n",
      "('sat', 'down', 'on')   1.1085798481005716e-12  \n",
      "('stood', 'for', 'a')   7.391534743898708e-13  \n",
      "('sat', 'shivering', 'on')   7.391534743898708e-13  \n",
      "('stood', 'a', 'few')   7.391534743898708e-13  \n",
      "('stood', 'glaring', 'over')   7.391534743898708e-13  \n",
      "('stood', 'reading', 'away')   7.391534743898708e-13  \n",
      "('sat', 'with', 'bleeding')   7.391534743898708e-13  \n",
      "('stood', 'in', 'the')   7.391200572609594e-13  \n",
      "\n",
      "\n",
      "\n",
      "Prediction of 10  highest probability for str2\n",
      "\n",
      "('to', 'his', 'heels')   1.6618930718825718e-12  \n",
      "('to', 'rest', 'by')   1.1086800904233945e-12  \n",
      "('to', 'earth', 'again')   1.1086800904233945e-12  \n",
      "('to', 'breakfast', 'on')   1.1086299669960052e-12  \n",
      "('on', 'a', 'day')   1.1083794517957985e-12  \n",
      "('on', 'a', 'little')   1.1083794517957985e-12  \n",
      "('on', 'a', 'poor')   1.1083794517957985e-12  \n",
      "('on', 'a', 'low')   1.1083794517957985e-12  \n",
      "('on', 'a', 'bread')   1.1083794517957985e-12  \n",
      "('on', 'a', 'rough')   1.1083794517957985e-12  \n",
      "\n",
      "\n",
      "\n",
      "Prediction of 10  highest probability for str3\n",
      "\n",
      "('the', 'side', 'of')   4.609914189308416e-12  \n",
      "('the', 'parish', 'authorities')   2.211659079836568e-12  \n",
      "('the', 'direction', 'of')   1.843965675723366e-12  \n",
      "('the', 'board', 'in')   1.84221689375208e-12  \n",
      "('the', 'very', 'first')   1.474439386557712e-12  \n",
      "('the', 'door', 'of')   1.4743061641148337e-12  \n",
      "('the', 'workhouse', 'authorities')   1.4743061641148337e-12  \n",
      "('the', 'board', 'had')   1.473773515001664e-12  \n",
      "('the', 'board', 'were')   1.473773515001664e-12  \n",
      "('the', 'arm', 'and')   1.1065294636742792e-12  \n",
      "\n",
      "\n",
      "Final Sentences on the basis of highest probability--------------------------\n",
      "\n",
      "For testSent1\n",
      "\n",
      "Bigram predicted sentence:      There was a sudden jerk, a terrific convulsion of the limbs; and there he was in the \n",
      "\n",
      "Trigram predicted sentence:     There was a sudden jerk, a terrific convulsion of the limbs; and there he sat down to\n",
      "\n",
      "\n",
      "\n",
      "For testSent2\n",
      "\n",
      "Bigram predicted sentence:    They made room for the stranger, but he sat down the old gentleman \n",
      "\n",
      "Trigram predicted sentence:   They made room for the stranger, but he sat down to his heels\n",
      "\n",
      "\n",
      "\n",
      "For testSent3\n",
      "\n",
      "Bigram predicted sentence:    The hungry and destitute situation of the infant orphan was duly reported by the old gentleman \n",
      "\n",
      "Trigram predicted sentence:   The hungry and destitute situation of the infant orphan was duly reported by the side of\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the smoothed models now we will predict the next 3 most possible word sequences for testSent1, testSent2 and testSent3\n",
    "\n",
    "#Splitting sentence into list of words\n",
    "test1 = testSent1.split(\" \")\n",
    "test2 = testSent2.split(\" \")\n",
    "test3 = testSent3.split(\" \")\n",
    "\n",
    "\n",
    "#Will contain the final probability of predicted three words for each sentence and both smoothed bigram and trigram model\n",
    "\n",
    "#For bigram model, predicting words from last one word of sentence\n",
    "#For trigram model, predicting words from last two word of sentence\n",
    "#probxy (x- sentence no, y-ngram model)\n",
    "\n",
    "prob12={}\n",
    "prob22={}\n",
    "prob32={}\n",
    "\n",
    "prob13={}\n",
    "prob23={}\n",
    "prob33={}\n",
    "\n",
    "\n",
    "# For storing the predicted sequence (next three words) with highest probability\n",
    "h12 = []\n",
    "h22 = []\n",
    "h32 = []\n",
    "h13 = []\n",
    "h23 = []\n",
    "h33 = []\n",
    "\n",
    "prob12[tuple(test1[-1:])]=1 \n",
    "prob22[tuple(test2[-1:])]=1\n",
    "prob32[tuple(test3[-1:])]=1\n",
    "\n",
    "prob13[tuple(test1[-2:])]=1 \n",
    "prob23[tuple(test2[-2:])]=1\n",
    "prob33[tuple(test3[-2:])]=1\n",
    "\n",
    "for i in range(3): # looping 3 times to predict next 3 words\n",
    "    #temporary variables for storing  probabilities of new sequence \n",
    "    prob12t={} #For bigram model\n",
    "    prob22t={}\n",
    "    prob32t={}\n",
    "    \n",
    "    prob13t={} #For trigram model\n",
    "    prob23t={}\n",
    "    prob33t={}\n",
    "    \n",
    "    for key in unigrams_freqDist.keys():\n",
    "\n",
    "    #For 1st Sentence\n",
    "        for seq in prob12.keys():\n",
    "            seqt = seq+(key,)\n",
    "            seql = list(seqt)\n",
    "            bigram = tuple(seql[-2:])\n",
    "            if bigram in bigram_prob:\n",
    "                prob = bigram_prob[bigram] #Calculating bigram probability\n",
    "                prob12t[seqt]=prob12[seq]*prob\n",
    "                    \n",
    "                \n",
    "        for seq in prob13.keys():\n",
    "            seqt = seq+(key,)\n",
    "            seql = list(seqt)\n",
    "            tgram = tuple(seql[-3:])\n",
    "            if tgram in trigram_prob:\n",
    "                prob = trigram_prob[tgram] #Calculating trigram probability\n",
    "                prob13t[seqt]=prob13[seq]*prob\n",
    "        \n",
    "                \n",
    "    #For 2nd Sentence            \n",
    "        for seq in prob22.keys():\n",
    "            seqt = seq+(key,)\n",
    "            seql = list(seqt)\n",
    "            bigram = tuple(seql[-2:])\n",
    "            if bigram in bigram_prob:\n",
    "                prob = bigram_prob[bigram] #Calculating bigram probability\n",
    "                prob22t[seqt]=prob22[seq]*prob\n",
    "        for seq in prob23.keys():\n",
    "            seqt = seq+(key,)\n",
    "            seql = list(seqt)\n",
    "            tgram = tuple(seql[-3:])\n",
    "            if tgram in trigram_prob:\n",
    "                prob = trigram_prob[tgram] #Calculating trigram probability\n",
    "                prob23t[seqt]=prob23[seq]*prob\n",
    "        \n",
    "                    \n",
    "    #For 3rd Sentence\n",
    "        for seq in prob32.keys():\n",
    "            seqt = seq+(key,)\n",
    "            seql = list(seqt)\n",
    "            bigram = tuple(seql[-2:])\n",
    "            if bigram in bigram_prob:\n",
    "                prob = bigram_prob[bigram] #Calculating bigram probability\n",
    "                prob32t[seqt]=prob32[seq]*prob\n",
    "        for seq in prob33.keys():\n",
    "            seqt = seq+(key,)\n",
    "            seql = list(seqt)\n",
    "            tgram = tuple(seql[-3:])\n",
    "            if tgram in trigram_prob:\n",
    "                prob = trigram_prob[tgram] #Calculating trigram probability\n",
    "                prob33t[seqt]=prob33[seq]*prob\n",
    "        \n",
    "    \n",
    "                \n",
    "    prob12=prob12t\n",
    "    prob22=prob22t\n",
    "    prob32=prob32t  \n",
    "    prob13=prob13t\n",
    "    prob23=prob23t\n",
    "    prob33=prob33t  \n",
    "            \n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "print(\"Bigram Predictions:-------------------------------------------------------------\\n\")\n",
    "o = Counter(prob12)\n",
    "pred = o.most_common(10)\n",
    "j = 0\n",
    "print(\"\\nPrediction of 10  highest probability for str1\\n\")\n",
    "for i in pred:\n",
    "  print(i[0][-3:],\" \",i[1],\" \")\n",
    "  j = j+1\n",
    "  if j==1 :\n",
    "    h12=list(i[0][-3:])\n",
    "    \n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "o = Counter(prob22)\n",
    "pred = o.most_common(10)\n",
    "j = 0\n",
    "print(\"\\nPrediction of 10  highest  probability for str2\\n\")\n",
    "for i in pred:\n",
    "  print(i[0][-3:],\" \",i[1],\" \")\n",
    "  j = j+1\n",
    "  if j==1 :\n",
    "    h22=list(i[0][-3:])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "o = Counter(prob32)\n",
    "pred = o.most_common(10)\n",
    "j = 0\n",
    "print(\"\\nPrediction  of 10  highest probability for str3\\n\")\n",
    "for i in pred:\n",
    "  print(i[0][-3:],\" \",i[1],\" \")\n",
    "  j = j+1\n",
    "  if j==1 :\n",
    "    h32=list(i[0][-3:])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Trigram Predictions:-------------------------------------------------------------\\n\")\n",
    "o = Counter(prob13)\n",
    "pred = o.most_common(10)\n",
    "j = 0\n",
    "print(\"\\nPrediction of 10 highest probability for str1\\n\")\n",
    "for i in pred:\n",
    "  print(i[0][-3:],\" \",i[1],\" \")\n",
    "  j = j+1\n",
    "  if j==1 :\n",
    "    h13=list(i[0][-3:])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "o = Counter(prob23)\n",
    "pred = o.most_common(10)\n",
    "j = 0\n",
    "print(\"\\nPrediction of 10  highest probability for str2\\n\")\n",
    "for i in pred:\n",
    "  print(i[0][-3:],\" \",i[1],\" \")\n",
    "  j = j+1\n",
    "  if j==1 :\n",
    "    h23=list(i[0][-3:])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "o = Counter(prob33)\n",
    "pred = o.most_common(10)\n",
    "j = 0\n",
    "print(\"\\nPrediction of 10  highest probability for str3\\n\")\n",
    "for i in pred:\n",
    "  print(i[0][-3:],\" \",i[1],\" \")\n",
    "  j = j+1\n",
    "  if j==1 :\n",
    "    h33=list(i[0][-3:])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Final Sentences on the basis of highest probability--------------------------\\n\")\n",
    "\n",
    "print(\"For testSent1\\n\")\n",
    "s12 = testSent1+' '+h12[0]+' '+h12[1]+' '+h12[2]\n",
    "print(\"Bigram predicted sentence:     \",s12,'\\n')\n",
    "s13 = testSent1+' '+h13[0]+' '+h13[1]+' '+h13[2]\n",
    "print(\"Trigram predicted sentence:    \",s13)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"For testSent2\\n\")\n",
    "s22 = testSent2+' '+h22[0]+' '+h22[1]+' '+h22[2]\n",
    "print(\"Bigram predicted sentence:   \",s22,'\\n')\n",
    "s23 = testSent2+' '+h23[0]+' '+h23[1]+' '+h23[2]\n",
    "print(\"Trigram predicted sentence:  \",s23)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"For testSent3\\n\")\n",
    "s32 = testSent3+' '+h32[0]+' '+h32[1]+' '+h32[2]\n",
    "print(\"Bigram predicted sentence:   \",s32,'\\n')\n",
    "s33 = testSent3+' '+h33[0]+' '+h33[1]+' '+h33[2]\n",
    "print(\"Trigram predicted sentence:  \",s33)\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxfeaacTdO6h"
   },
   "source": [
    "Check the presence of these sentences in the original corpus at https://www.gutenberg.org/files/730/730-0.txt . How did your smoothed models perform in comparison to the original sentences? Compare them below.\n",
    "\n",
    "Did you notice something special about testSent3, in comparison to testSent1 and testSent2? If yes, what is it? Can you explain it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFMkW9hKecxK"
   },
   "source": [
    "  - - - - - - - - - -\n",
    "  ## Answer here ## \n",
    "\n",
    "None of the models are predicting correct sequences for the given Sentences with the highest probablity as they are just looking one or two previous words. But the predictions are reasonable on the basis of their repeated presence in the corpus.\n",
    "The trigram model is performing better in terms of context than the bigram model. The output of trigram is more meaningful as it is accounting for one more words to find probability which is giving more contextual meaning.\n",
    "\n",
    "TestSent1 and TestSent2 are not in the Corpus as we have extracted Chapter 1-10 and they are out of these chapters.\n",
    "\n",
    "TestSent3 is in Chapter-2(The hungry and destitute situation of the infant orphan was duly reported by the workhouse authorities). These three  words' sequence just after the TestSent3 has been predicted by trigram models which is also shown in the output(in bold) among the top 10 sequence.\n",
    "   - - - - - - - - - -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVBMcaAJXR9S"
   },
   "source": [
    "Which of the three models you generated above (unigram, bigram, trigram) is better in terms of **perplexity**, for the three test sentences (unseen data)? Write a piece of code to justify your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "AAPa1OVZX8uN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram predicted sentence:  There was a sudden jerk, a terrific convulsion of the limbs; and there he was in the\n",
      "Perplexity with bigram model:  1162.0139781566184\n",
      "\n",
      "Trigram predicted sentence:  There was a sudden jerk, a terrific convulsion of the limbs; and there he sat down to\n",
      "Perplexity with trigram model:  3433.743989727209\n",
      "-----------------------------------------------------------------------------------\n",
      "Bigram predicted sentence:  They made room for the stranger, but he sat down the old gentleman\n",
      "Perplexity with bigram model:  953.5758234682314\n",
      "\n",
      "Trigram predicted sentence:  They made room for the stranger, but he sat down to his heels\n",
      "Perplexity with trigram model:  3109.66140900433\n",
      "-----------------------------------------------------------------------------------\n",
      "Bigram predicted sentence:  The hungry and destitute situation of the infant orphan was duly reported by the old gentleman\n",
      "Perplexity with bigram model:  884.1816576101656\n",
      "\n",
      "Trigram predicted sentence:  The hungry and destitute situation of the infant orphan was duly reported by the side of\n",
      "Perplexity with trigram model:  2002.825830985676\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "  # Function to compute the perplexity score using Smoothed Probabilities\n",
    "def perplexity(model, sentence):\n",
    "\n",
    "    probability = 1\n",
    "    perplexity = None\n",
    "    words = sentence.split(' ')\n",
    "\n",
    "    if model == \"unigram\":\n",
    "\n",
    "        total_word_occurances = 0\n",
    "        for key, value in unigrams_freqDist.items():\n",
    "            total_word_occurances = total_word_occurances + value\n",
    "\n",
    "        for word in ngrams(word_tokenize(sentence),1):\n",
    "            \n",
    "            # Count unigram occurances\n",
    "            if word not in unigrams_freqDist.keys():\n",
    "                count_unigram = 0\n",
    "            else:\n",
    "                count_unigram = unigrams_freqDist[word]\n",
    "            \n",
    "            # Compute the smoothed probability of the sentence\n",
    "            probability = probability * ((count_unigram + 1) / (total_word_occurances + len(unigrams_freqDist.keys())))\n",
    "\n",
    "            # Calculate the perplexity score\n",
    "            perplexity = (1/probability) ** (1/len(words))\n",
    "\n",
    "    elif model == \"bigram\":\n",
    "        \n",
    "        for bigram in ngrams(word_tokenize(sentence),2):\n",
    "\n",
    "            # Counting bigram occurances\n",
    "            if bigram not in bigrams_freqDist.keys():\n",
    "                count_bigram = 0\n",
    "            else:\n",
    "                count_bigram = bigrams_freqDist[bigram]\n",
    "\n",
    "            # Counting unigram occurances\n",
    "            if bigram[0] not in unigrams_freqDist.keys():\n",
    "                count_unigram = 0\n",
    "            else:\n",
    "                count_unigram = unigrams_freqDist[bigram[0]]\n",
    "\n",
    "            # Computing the smoothed probability of the sentence\n",
    "            probability = probability * ((count_bigram + 1) / (count_unigram + len(unigrams_freqDist)))\n",
    "\n",
    "        #in order to count number of bigrams\n",
    "        count = 0\n",
    "        for i in ngrams(word_tokenize(sentence),2) :\n",
    "          count = count + 1\n",
    "\n",
    "        # Calculate the perplexity score\n",
    "        perplexity = (1/probability) ** (1/(count))\n",
    "    \n",
    "    elif model == \"trigram\":\n",
    "        \n",
    "        for trigram in ngrams(word_tokenize(sentence),3):\n",
    "            \n",
    "            bigram = tuple((trigram[0], trigram[1]))\n",
    "            \n",
    "            # Computing bigram occurances\n",
    "            if bigram not in bigrams_freqDist.keys():\n",
    "                count_bigram = 0\n",
    "            else:\n",
    "                count_bigram = bigrams_freqDist[bigram]\n",
    "\n",
    "            # Computing trigram occurances\n",
    "            if trigram not in trigrams_freqDist.keys():\n",
    "                count_trigram = 0\n",
    "            else:\n",
    "                count_trigram = trigrams_freqDist[trigram]\n",
    "\n",
    "            # Computing the probability\n",
    "            probability = probability * ((count_trigram + 1) / (count_bigram + len(unigrams_freqDist)))\n",
    "\n",
    "        #in order to count number of bigrams\n",
    "        count = 0\n",
    "        for i in ngrams(word_tokenize(sentence),3) :\n",
    "          count = count + 1\n",
    "\n",
    "        # Calculate the perplexity score\n",
    "        perplexity = (1/probability) ** (1/count)\n",
    "\n",
    "\n",
    "    # Return the perplexity score\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# For testSent1\n",
    "print(\"Bigram predicted sentence: \", s12)\n",
    "print(\"Perplexity with bigram model: \", perplexity(\"bigram\", s12))\n",
    "print()\n",
    "print(\"Trigram predicted sentence: \", s13)\n",
    "print(\"Perplexity with trigram model: \", perplexity(\"trigram\",s13))\n",
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "\n",
    "# For testSent2\n",
    "\n",
    "print(\"Bigram predicted sentence: \", s22)\n",
    "print(\"Perplexity with bigram model: \", perplexity(\"bigram\", s22))\n",
    "print()\n",
    "print(\"Trigram predicted sentence: \", s23)\n",
    "print(\"Perplexity with trigram model: \", perplexity(\"trigram\", s23))\n",
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "\n",
    "# For testSent3\n",
    "\n",
    "print(\"Bigram predicted sentence: \", s32)\n",
    "print(\"Perplexity with bigram model: \", perplexity(\"bigram\", s32))\n",
    "print()\n",
    "print(\"Trigram predicted sentence: \", s33)\n",
    "print(\"Perplexity with trigram model: \", perplexity(\"trigram\", s33))\n",
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Here\n",
    "\n",
    "The perplexity score using trigram model is higher for all the three sentences, meaning the probability is lower for that model. So, in this case, bigram is a better model as compared to the trigram model. \n",
    "The trigram model even though captures more context, however, fails to perform better than the bigram model in terms of perplexity."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_CS60075_A21_Assn1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
